{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c556a861-1ff9-45ab-ab0c-b869c9c50d3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T15:37:21.148896Z",
     "start_time": "2024-12-15T15:37:21.122983Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import functools\n",
    "import pickle\n",
    "\n",
    "from selfcheckgpt.modeling_selfcheck import SelfCheckMQAG, SelfCheckBERTScore, SelfCheckNgram\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import statistics\n",
    "import spacy\n",
    "\n",
    "from result_collector import trex_data_to_question_template, answer_trivia, answer_trex, load_data, model_dir\n",
    "\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62dfa62-5a05-45d2-b15e-fdcd66d1c862",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T15:37:23.099472Z",
     "start_time": "2024-12-15T15:37:23.077324Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "org=\"tiiuae\"\n",
    "model_name = \"falcon-7b\"\n",
    "repo = f\"{org}/{model_name}\"\n",
    "\n",
    "# Data related params\n",
    "dataset_name = \"trivia_qa\"\n",
    "\n",
    "# GPU\n",
    "gpu = \"0\"\n",
    "device = torch.device(f\"cuda:{gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# SelfCheckGPT\n",
    "self_checkgpt_temperature = 1.0\n",
    "selfcheckgpt_n_trials = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63cbdaf-5e7f-4f12-838f-52365e1220d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T15:36:59.884187Z",
     "start_time": "2024-12-15T15:36:47.977073Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_data(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9392d8a9-c11c-4df4-8992-f88a4c86a58e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T15:39:05.968990Z",
     "start_time": "2024-12-15T15:37:44.744213Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(repo)\n",
    "model = AutoModelForCausalLM.from_pretrained(repo, cache_dir=model_dir, torch_dtype=torch.bfloat16, trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338286c3-6406-417a-a306-16a9272b7928",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selfcheck_bertscore = SelfCheckBERTScore(rescale_with_baseline=True)\n",
    "selfcheck_ngram = SelfCheckNgram(n=1) # n=1 means Unigram, n=2 means Bigram, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f92d8fe-0427-4c1d-a8b8-74fee3758f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_responses(question, str_response, tokenizer):\n",
    "\n",
    "    # generate several responses to the question and (self)check them against the zero temp response\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\").input_ids.to(device)\n",
    "    start_pos = inputs.size(dim=-1)\n",
    "\n",
    "    hitemp_str_responses = []\n",
    "    for i in range(0, selfcheckgpt_n_trials):\n",
    "        model_outputs = model.generate(\n",
    "            inputs, do_sample=True, temperature=self_checkgpt_temperature, max_new_tokens=100, return_dict_in_generate=True, output_scores=True\n",
    "        )\n",
    "        generated_tokens_ids = model_outputs.sequences[0]\n",
    "        hitemp_str_responses.append(tokenizer.decode(generated_tokens_ids[start_pos:]).replace(\"\\n\", \" \").strip())\n",
    "\n",
    "    selfcheck_scores_bert_overall = []\n",
    "    selfcheck_scores_bert_average = []\n",
    "    selfcheck_ngram_overall = []\n",
    "    \n",
    "    sentences = [str_response]\n",
    "    overall_bertscore = selfcheck_bertscore.predict(\n",
    "        sentences = sentences,                          # list of sentences\n",
    "        sampled_passages = hitemp_str_responses, # list of sampled passages\n",
    "    )\n",
    "    selfcheck_scores_bert_overall.append(overall_bertscore[0])\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    sentences = [sent for sent in nlp(str_response).sents]\n",
    "    sentences = [sent.text.strip() for sent in sentences if len(sent) > 3]\n",
    "    all_bertscores = selfcheck_bertscore.predict(\n",
    "        sentences = sentences,                          # list of sentences\n",
    "        sampled_passages = hitemp_str_responses, # list of sampled passages\n",
    "    )\n",
    "    average_bertscore = statistics.mean(all_bertscores)\n",
    "    selfcheck_scores_bert_average.append(average_bertscore)\n",
    "      \n",
    "    \n",
    "    sent_scores_ngram = selfcheck_ngram.predict(\n",
    "        sentences = sentences,   \n",
    "        passage = str_response,\n",
    "        sampled_passages = hitemp_str_responses,\n",
    "    )\n",
    "    selfcheck_ngram_overall.append(sent_scores_ngram)\n",
    "    \n",
    "    return hitemp_str_responses, selfcheck_scores_bert_overall, selfcheck_scores_bert_average, selfcheck_ngram_overall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ba328f-3eb7-4703-aab2-470ccb3cdcc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selfcheck_dict = {\n",
    "        'question': [],\n",
    "        'response': [],\n",
    "        'str_response': [],\n",
    "        'start_pos': [],\n",
    "        'correct': [],\n",
    "        'hitemp_str_responses': [],\n",
    "        'selfcheck_scores_bert_overall': [],\n",
    "        'selfcheck_scores_bert_average': [],\n",
    "        'selfcheck_ngram_overall': []\n",
    "    }\n",
    "\n",
    "selfcheck_arr_overall = []\n",
    "selfcheck_arr_average = []\n",
    "selfcheck_ngram_average = []\n",
    "correct_arr = []\n",
    "\n",
    "if dataset_name in trex_data_to_question_template.keys():\n",
    "    question_asker = functools.partial(answer_trex, question_template=trex_data_to_question_template[dataset_name])\n",
    "elif dataset_name == \"trivia_qa\":\n",
    "    question_asker = answer_trivia\n",
    "else:\n",
    "    raise ValueError(f\"Unknown dataset name {dataset_name}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c00f8f2-51fa-4af7-8d50-7e5cac40c6c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx in tqdm(range(len(dataset))):\n",
    "\n",
    "    question, answers = dataset[idx]\n",
    "    response, str_response, logits, start_pos, correct = question_asker(question, answers, model, tokenizer)\n",
    "    hitemp_str_responses, selfcheck_scores_bert_overall, selfcheck_scores_bert_average, selfcheck_ngram_overall\\\n",
    "        = generate_responses(\n",
    "            question if dataset_name==\"trivia_qa\" else trex_data_to_question_template[dataset_name].substitute(source=question),\n",
    "            str_response, \n",
    "            tokenizer\n",
    "        )\n",
    "\n",
    "    selfcheck_dict['question'].append(question)\n",
    "    selfcheck_dict['response'].append(response)\n",
    "    selfcheck_dict['str_response'].append(str_response)\n",
    "    selfcheck_dict['start_pos'].append(start_pos)\n",
    "    selfcheck_dict['correct'].append(correct)\n",
    "    selfcheck_dict['hitemp_str_responses'].append(hitemp_str_responses)\n",
    "    selfcheck_dict['selfcheck_scores_bert_overall'].append(selfcheck_scores_bert_overall)\n",
    "    selfcheck_dict['selfcheck_scores_bert_average'].append(selfcheck_scores_bert_average)\n",
    "    selfcheck_dict['selfcheck_ngram_overall'].append(selfcheck_ngram_overall)\n",
    "\n",
    "    selfcheck_arr_overall.append(1.0-selfcheck_scores_bert_overall[0]) #bert score flipped\n",
    "    selfcheck_arr_average.append(1.0-selfcheck_scores_bert_average[0]) #bert score flipped\n",
    "    selfcheck_ngram_average.append(1.0-np.exp(-selfcheck_ngram_overall[0]['doc_level']['avg_neg_logprob']))\n",
    "    correct_arr.append(int(correct))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b1ff13-f25e-49b3-a14d-814d81f9689f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(selfcheck_arr_overall)\n",
    "#print(correct_arr)\n",
    "roc_score = roc_auc_score(correct_arr, selfcheck_arr_overall)\n",
    "print(f\"AUROC for self check overall: {roc_score}\")\n",
    "\n",
    "#print(selfcheck_arr_average)\n",
    "#print(correct_arr)\n",
    "roc_score = roc_auc_score(correct_arr, selfcheck_arr_average)\n",
    "print(f\"AUROC for self check average: {roc_score}\")\n",
    "\n",
    "roc_score = roc_auc_score(correct_arr, selfcheck_ngram_average)\n",
    "print(f\"AUROC for self check ngram: {roc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f64d92-b9a6-4772-ad75-685320e5259d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f\"selfcheck_{model_name}_{dataset_name}_{gpu}.pickle\", \"wb\") as outfile:\n",
    "        outfile.write(pickle.dumps(selfcheck_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e016206-3f46-420a-a94e-827fa28ceb4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selfcheck_dict['hitemp_str_responses'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ac764a-6b59-4680-be92-7ceb0cbeb78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selfcheck_dict['hitemp_str_responses'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf95294a-bd6a-463a-b733-0486eb75927a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "22a0ee0a-c1f5-4450-9830-10a1383f28fe",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
